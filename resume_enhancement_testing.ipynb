{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded NEW llm_api with OpenAI SDK v1.x syntax\n",
      "✅ Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Importing main.py functions\n",
    "from main import (\n",
    "    setup_environment,\n",
    "    process_resume,\n",
    "    split_resume_into_sections,\n",
    "    process_job_posting,\n",
    "    extract_keywords,\n",
    "    calculate_keyword_match,\n",
    "    filter_relevant_keywords,\n",
    "    enhance_section\n",
    ")\n",
    "\n",
    "from parsing_module import extract_headers_with_pdfplumber\n",
    "\n",
    "\n",
    "\n",
    "setup_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HEADER CANDIDATE] 'Neal Iyer'\n",
      "[HEADER CANDIDATE] 'Professional Summary'\n",
      "[NORMALIZED] 'Professional Summary' → 'summary'\n",
      "[HEADER CANDIDATE] 'Professional Skills'\n",
      "[NORMALIZED] 'Professional Skills' → 'skills'\n",
      "[HEADER CANDIDATE] 'Workforce Planning'\n",
      "[HEADER CANDIDATE] 'Optimization'\n",
      "[HEADER CANDIDATE] 'Communication'\n",
      "[HEADER CANDIDATE] 'Proactive Problem-Solving\n",
      "Experience'\n",
      "[HEADER CANDIDATE] 'Budget Analyst'\n",
      "[HEADER CANDIDATE] 'Budget Analyst'\n",
      "[HEADER CANDIDATE] 'Cresa\n",
      "Accounting Analyst'\n",
      "[HEADER CANDIDATE] 'FrontStream\n",
      "Fund Accountant'\n",
      "[HEADER CANDIDATE] 'Education and Certifications'\n",
      "[normalize_section_name] GPT returned: 'education' for 'Education and Certifications'\n",
      "[NORMALIZED] 'Education and Certifications' → 'education'\n",
      "[HEADER CANDIDATE] 'Flatiron Data Science Bootcamp'\n",
      "[HEADER CANDIDATE] 'HackerRank Intermediate SQL Certification'\n",
      "[HEADER CANDIDATE] 'Projects'\n",
      "[FORCED-NORMALIZED] 'Projects' → 'projects' (from font size match)\n",
      "[INJECTED] Forcing section 'experience' from pdfplumber headers\n",
      "[INJECTED] Forcing section 'certifications' from pdfplumber headers\n",
      "\n",
      "[DEBUG] Headers from pdfplumber: ['Neal', 'Iyer', 'Professional', 'Summary', 'Skills', 'Experience', 'Experience', 'Education', 'and', 'Certifications', 'Projects']\n",
      "✅ Resume and job posting processed successfully.\n",
      "Found Resume Sections: ['summary', 'skills', 'education', 'projects', 'experience', 'certifications']\n"
     ]
    }
   ],
   "source": [
    "resume_file = \"docs/sample_resume.pdf\"\n",
    "job_input = \"\"\"\n",
    "[Big Data Tools Developer\n",
    "\n",
    "We build, improve, and maintain one of the highest scaling platforms in the world. Our amazing team of Engineers work on next generation Big Data Platforms that transform how users connect with each other every single day. Yahoo's Big Data Platform drives some of the most demanding applications in the industry. The system handles billions of requests a day and runs on some of the largest Hadoop clusters ever built! 50,000 nodes strong and several multi-thousand node clusters bring scalable computing to a whole new level. We work on problems that cover a wide spectrum - from web services to operating systems and networking layers. Our biggest challenges ahead are designing efficient cloud native big data platforms.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Job Monitoring: Overseeing the execution of various data jobs, ensuring they adhere to SLAs and do not encounter issues.\n",
    "Data Orchestration: Utilizing tools like Airflow to manage the scheduling, execution, and monitoring of data workflows across cloud platforms such as AWS and GCP.\n",
    "Query Execution and Optimization: Designing and optimizing queries to run efficiently on platforms such as BigQuery, Hive, Pig, and Spark, ensuring high performance and scalability.\n",
    "Integration and Support: Collaborating with different teams to integrate data flows, provide support for query executions, and handle credentials for secure data operations.\n",
    "Feature Development: Implementing new features to support advanced query capabilities, including federated queries and lineage tracking.\n",
    "\n",
    "Required Skills and Qualifications:\n",
    "\n",
    "Educational Background: A Bachelor's or Master’s degree in Computer Science or equivalent work experience.\n",
    "Programming Languages: Proficiency in Python is essential for scripting and workflow management; experience with Java and C++ is preferred for backend data operations.\n",
    "Data Management: Knowledge of data structures, algorithms, and database management systems like SQL, HBase, and BigQuery.\n",
    "Cloud Technologies: Experience with cloud services, especially AWS (EMR, Glue, S3) and GCP (Dataproc, BigQuery).\n",
    "Agile Methodology: Comfortable working in an Agile environment with regular sprints, planning, and retrospectives.\n",
    "System Design: Ability to design large-scale, distributed systems that are highly available and resilient.\n",
    "OS: Some experience working with Linux/Unix operating systems\n",
    "\n",
    "Preferred Qualifications:\n",
    "\n",
    "Experience with development and deployment on public cloud platforms such as AWS, GCP, Azure, or others\n",
    "Experiencing developing containerized applications and working with container orchestration services\n",
    "Experience with Apache Hadoop, Presto, Hive, Oozie, Pig, Storm, Spark, Jupyter\n",
    "Understanding of data structures & algorithms\n",
    "Knowledge of JVM internals and its performance tuning\n",
    "Excellent debugging/testing skills, and excellent analytical and problem solving skills\n",
    "Experience with continuous integration tools such as Jenkins and Hudson\n",
    "Strong verbal and written communication skills to collaborate effectively with cross-functional teams.]\n",
    "\"\"\"\n",
    "\n",
    "# Extract text from resume and job posting\n",
    "resume_text = process_resume(resume_file)\n",
    "job_text = process_job_posting(job_input)\n",
    "\n",
    "# Parse sections from resume\n",
    "sections = split_resume_into_sections(resume_text, pdf_path=resume_file)\n",
    "\n",
    "# DEBUG: Print pdfplumber headers directly\n",
    "pdf_headers = extract_headers_with_pdfplumber(resume_file)\n",
    "print(\"\\n[DEBUG] Headers from pdfplumber:\", pdf_headers)\n",
    "\n",
    "\n",
    "# Extract overall keywords\n",
    "resume_keywords = extract_keywords(resume_text)\n",
    "job_keywords = extract_keywords(job_text)\n",
    "\n",
    "print(\"✅ Resume and job posting processed successfully.\")\n",
    "print(f\"Found Resume Sections: {list(sections.keys())}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
